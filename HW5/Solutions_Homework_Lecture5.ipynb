{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Lecture5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA and Logistic Classification and Feature Development for MNIST Image sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T02:53:48.522285Z",
     "start_time": "2018-03-02T02:53:43.783853Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "from E4525_ML import mnist\n",
    "from E4525_ML.multiclass_logistic import LogisticGDClassifier\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-15T11:06:16.497625Z",
     "start_time": "2018-02-15T11:06:16.466372Z"
    }
   },
   "source": [
    "### Random Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T02:53:48.537898Z",
     "start_time": "2018-03-02T02:53:48.522285Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed=458\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T02:53:48.561796Z",
     "start_time": "2018-03-02T02:53:48.537898Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir=r\"../../raw/mnist/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 1.0 </div>\n",
    "Read MNIST data set and labels,  also read the MNMIST test data set and test labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T02:53:49.201245Z",
     "start_time": "2018-03-02T02:53:48.563202Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n"
     ]
    }
   ],
   "source": [
    "    images_filename=data_dir+\"train-images-idx3-ubyte.gz\"\n",
    "    labels_filename=data_dir+\"train-labels-idx1-ubyte.gz\"\n",
    "\n",
    "    test_images_filename=data_dir+\"t10k-images-idx3-ubyte.gz\"\n",
    "    test_labels_filename=data_dir+\"t10k-labels-idx1-ubyte.gz\"\n",
    "\n",
    "    images=mnist.read_images(images_filename)\n",
    "    labels=mnist.read_labels(labels_filename)\n",
    "    \n",
    "    test_images=mnist.read_images(test_images_filename)\n",
    "    test_labels=mnist.read_labels(test_labels_filename)\n",
    "    \n",
    "    print(images.shape,labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T22:03:06.235419Z",
     "start_time": "2018-01-21T22:03:06.219795Z"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 1.2 </div>\n",
    "Use `skelearn`'s `train_test_split` function to separate the MNIST samples into  a 15% validation set and a  training sample.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T02:53:49.370644Z",
     "start_time": "2018-03-02T02:53:49.201245Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((51000, 28, 28), (9000, 28, 28))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_train,images_val,labels_train,labels_val=train_test_split(images,labels,test_size=0.15)\n",
    "images_train.shape,images_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-21T22:09:15.766264Z",
     "start_time": "2018-01-21T22:09:15.750639Z"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 2.1 </div>\n",
    "fit an LDA model on the training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T02:53:56.122416Z",
     "start_time": "2018-03-02T02:53:49.370644Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:387: UserWarning: Variables are collinear.\n",
      "  warnings.warn(\"Variables are collinear.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
       "              solver='svd', store_covariance=False, tol=0.0001)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=LinearDiscriminantAnalysis()\n",
    "model.fit(images_train.reshape(len(images_train),-1),labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-27T23:22:08.665966Z",
     "start_time": "2018-02-27T23:22:08.656899Z"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 2.2 </div>\n",
    "Compute model accuracy on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T02:53:56.250935Z",
     "start_time": "2018-03-02T02:53:56.124604Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Training accuracy', 0.87211764705882355)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred=model.predict(images_train.reshape(len(images_train),-1))\n",
    "\"Training accuracy\",np.mean(Y_pred==labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-27T23:22:33.457547Z",
     "start_time": "2018-02-27T23:22:33.449526Z"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 2.3 </div>\n",
    "Compute accuracy of the model on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T02:53:56.284022Z",
     "start_time": "2018-03-02T02:53:56.253442Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Validation accuracy', 0.86477777777777776)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred=model.predict(images_val.reshape(len(images_val),-1))\n",
    "\"Validation accuracy\",np.mean(Y_pred==labels_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 3.1 </div>\n",
    "\n",
    "Use the `LogisticGDClassifier` class from `E4525_ML.multiclass_logistic` module to fit a logistic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T02:54:18.808353Z",
     "start_time": "2018-03-02T02:53:56.286028Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t 0 Loss = 2371.93402487 Train_Accuracy 0.07 Evaluation Loss = 2361.71508727 Accuracy = 0.074\n",
      "\t 10 Loss = 241.973233903 Train_Accuracy 0.923 Evaluation Loss = 286.917511869 Accuracy = 0.917\n",
      "\t 20 Loss = 246.977401865 Train_Accuracy 0.927 Evaluation Loss = 270.820700789 Accuracy = 0.929\n",
      "\t 30 Loss = 211.514657894 Train_Accuracy 0.936 Evaluation Loss = 280.82263712 Accuracy = 0.921\n",
      "\t 40 Loss = 297.338538704 Train_Accuracy 0.922 Evaluation Loss = 291.958969499 Accuracy = 0.927\n",
      "\t 50 Loss = 215.526658685 Train_Accuracy 0.933 Evaluation Loss = 322.493619841 Accuracy = 0.919\n",
      "\t 60 Loss = 230.03199764 Train_Accuracy 0.93 Evaluation Loss = 241.338675769 Accuracy = 0.936\n",
      "\t 70 Loss = 224.457288023 Train_Accuracy 0.935 Evaluation Loss = 319.988976253 Accuracy = 0.913\n",
      "\t 80 Loss = 216.871540811 Train_Accuracy 0.939 Evaluation Loss = 249.979327898 Accuracy = 0.931\n",
      "\t 90 Loss = 251.477731453 Train_Accuracy 0.93 Evaluation Loss = 322.836290933 Accuracy = 0.904\n",
      "\t 99 Loss = 248.914335198 Train_Accuracy 0.93 Evaluation Loss = 297.1138059 Accuracy = 0.917\n"
     ]
    }
   ],
   "source": [
    "model=LogisticGDClassifier()\n",
    "model.fit(images_train.reshape(len(images_train),-1),labels_train,\n",
    "          # for display only\n",
    "         images_val.reshape(len(images_val),-1),labels_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 3.2 </div>\n",
    "Compute model accuracy in the training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T02:54:18.909580Z",
     "start_time": "2018-03-02T02:54:18.808353Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93462745098039213"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred=model.predict(images_train.reshape(len(images_train),-1))\n",
    "np.mean(Y_pred==labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T11:10:43.634394Z",
     "start_time": "2018-02-28T11:10:43.627379Z"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 3.3 </div>\n",
    "Compute model accuracy in the valuation data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T02:54:18.941595Z",
     "start_time": "2018-03-02T02:54:18.909580Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92100000000000004"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred=model.predict(images_val.reshape(len(images_val),-1))\n",
    "np.mean(Y_pred==labels_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering in one Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T02:54:18.953092Z",
     "start_time": "2018-03-02T02:54:18.945578Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N=50\n",
    "N_val=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T02:54:18.968123Z",
     "start_time": "2018-03-02T02:54:18.957105Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 10*(1-4*(np.abs(np.abs(x)-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T02:54:18.988127Z",
     "start_time": "2018-03-02T02:54:18.972126Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_sample(N):\n",
    "    X=np.random.uniform(-2,2,N)\n",
    "    eta=f(X)\n",
    "    eta.shape\n",
    "    theta=1/(1+np.exp(-eta))\n",
    "    Y= np.random.uniform(0,1,N)>theta\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 4.0 </div>\n",
    "Generate a training sample of variables $X$ and $Y$ with $N$ data points, and separate valuation and test samples\n",
    "with $N_val$ datap points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T02:54:19.004123Z",
     "start_time": "2018-03-02T02:54:18.988127Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train,Y_train=generate_sample(N)\n",
    "X_val,Y_val=generate_sample(N_val)\n",
    "X_test,Y_test=generate_sample(N_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T17:55:32.365942Z",
     "start_time": "2018-02-28T17:55:32.347942Z"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 4.1 </div>\n",
    "What is the proportion of positive class ($Y=1$) samples on the training data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T02:54:19.032124Z",
     "start_time": "2018-03-02T02:54:19.008125Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71999999999999997"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T17:39:04.058942Z",
     "start_time": "2018-02-28T17:39:04.050942Z"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 4.2 </div>\n",
    "Write a function able to generate the feature matrix\n",
    "$$\n",
    "    H_{i,d}= h_d(x_i)\n",
    "$$\n",
    "for $i=1,\\dots N$ and $d=1,\\dots D$\n",
    "\n",
    "where the functions $h_d(x)$ are defined as \n",
    "$$\n",
    "    h_d(x) = x^d\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T02:54:19.051637Z",
     "start_time": "2018-03-02T02:54:19.036124Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def polynomial_features(X,D):\n",
    "    features=[]\n",
    "    for d in range(1,D+1):\n",
    "        hd=X**d\n",
    "        features.append(hd)\n",
    "    return np.vstack(features).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-28T17:42:00.116942Z",
     "start_time": "2018-02-28T17:42:00.110942Z"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 4.3 </div>\n",
    "We will train  logistic regression models (use sklearn `LogisticRegression` class) over the training data you already generated. \n",
    "\n",
    "\n",
    "Use the valuation set you already generated to select the best value of $D$, also plot accuracy on the  training and valuation sets as a function of $D$.\n",
    "\n",
    "[HINT]\n",
    "1. You only need to consider the range $D=1,\\dots D=10$.\n",
    "2. Remember to disable regularization by setting the parameter $C$ of the `LogisticRegression` class to a very large number.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T02:54:19.096174Z",
     "start_time": "2018-03-02T02:54:19.054645Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.72 0.731\n",
      "2 0.72 0.731\n",
      "3 0.86 0.792\n",
      "4 0.96 0.891\n",
      "5 0.96 0.923\n",
      "6 0.96 0.915\n",
      "7 0.96 0.9\n",
      "8 0.96 0.909\n",
      "9 0.96 0.882\n",
      "10 0.96 0.898\n",
      "Best 5 0.923\n"
     ]
    }
   ],
   "source": [
    "train_accuracies=[]\n",
    "val_accuracies=[]\n",
    "\n",
    "Ds=range(1,11)\n",
    "for D in Ds:\n",
    "    H_train=polynomial_features(X_train,D)\n",
    "    model=LogisticRegression(C=1e40)\n",
    "    model.fit(H_train,Y_train)\n",
    "    Y_pred=model.predict(H_train)\n",
    "    train_accuracy=np.mean(Y_pred==Y_train)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    H_val=polynomial_features(X_val,D)\n",
    "    Y_pred=model.predict(H_val)\n",
    "    val_accuracy=np.mean(Y_pred==Y_val) \n",
    "    val_accuracies.append(val_accuracy)\n",
    "    print(D,train_accuracy,val_accuracy)\n",
    "val_accuracies=np.array(val_accuracies)\n",
    "best_idx=val_accuracies.argmax()\n",
    "best_D=Ds[best_idx]\n",
    "print(\"Best\",best_D,val_accuracies[best_idx])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T02:54:19.300275Z",
     "start_time": "2018-03-02T02:54:19.096174Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1d68f967128>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d68fbb68d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(Ds,val_accuracies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 4.4 </div>\n",
    "Use the test set you already generated to measure the accuracy for the optimal classifier you have found\n",
    "(do not use data from the  valuation set to train the classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T02:54:19.312275Z",
     "start_time": "2018-03-02T02:54:19.300275Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy 0.933\n"
     ]
    }
   ],
   "source": [
    "H_train=polynomial_features(X_train,best_D)\n",
    "model=LogisticRegression(C=1e40)\n",
    "model.fit(H_train,Y_train)\n",
    "H_test=polynomial_features(X_test,best_D)\n",
    "Y_pred=model.predict(H_test)\n",
    "test_accuracy=np.mean(Y_pred==Y_test) \n",
    "print(\"Test Accuracy\",test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering for MNIST sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 5.1 </div>\n",
    "In this problem we will use `mnist.ImageFeatureModel` class to find the optimal number of orientations $\\theta$  of the oriented gradients\n",
    "features for the MNIST data set.\n",
    "\n",
    "1. use `mnist.ImageFeatureModel` to generate image oriented gradient features.\n",
    "2. use  `LogisticGDClassifier` as the base model\n",
    "3. set the block size to 4 \n",
    "4. select the best number of orientations by performing  5-Fold cross validation on the full MNIST data set.\n",
    "5. Consider only [1,2,4,8] as possible values for the orientation\n",
    "6. Plot number of orientations vs validation accuracy\n",
    "\n",
    "[HINT] to `valiation_model` function below will be useful to perform cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T02:54:19.351788Z",
     "start_time": "2018-03-02T02:54:19.312275Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate_model(model,K,X,Y):\n",
    "    folder=KFold(K,shuffle=True)\n",
    "    folds=folder.split(X,Y)\n",
    "    val_error=0.0\n",
    "    fold_count=0\n",
    "    for fold in folds:\n",
    "        train_idx,val_idx=fold\n",
    "        x_train=X[train_idx]\n",
    "        y_train=Y[train_idx]\n",
    "        x_val=X[val_idx]\n",
    "        y_val=Y[val_idx]     \n",
    "        model.fit(x_train,y_train)\n",
    "        y_pred=model.predict(x_val)\n",
    "        val_err=np.mean(y_val==y_pred)\n",
    "        #print(y_pred)\n",
    "        val_error+=val_err\n",
    "        fold_count+=1\n",
    "        print(fold_count,val_err)\n",
    "    return val_error/K\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T03:14:27.026220Z",
     "start_time": "2018-03-02T02:54:19.356299Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t 0 Loss = 2284.24766048 Train_Accuracy 0.125 \n",
      "\t 10 Loss = 557.426414386 Train_Accuracy 0.836 \n",
      "\t 20 Loss = 511.526745603 Train_Accuracy 0.849 \n",
      "\t 30 Loss = 506.540144123 Train_Accuracy 0.861 \n",
      "\t 40 Loss = 503.296292931 Train_Accuracy 0.834 \n",
      "\t 50 Loss = 425.369758057 Train_Accuracy 0.873 \n",
      "\t 60 Loss = 482.066491927 Train_Accuracy 0.857 \n",
      "\t 70 Loss = 447.989698541 Train_Accuracy 0.839 \n",
      "\t 80 Loss = 476.333354796 Train_Accuracy 0.856 \n",
      "\t 90 Loss = 401.105346972 Train_Accuracy 0.876 \n",
      "\t 99 Loss = 405.630319225 Train_Accuracy 0.878 \n",
      "1 0.863833333333\n",
      "\t 0 Loss = 2290.99398618 Train_Accuracy 0.127 \n",
      "\t 10 Loss = 561.798978673 Train_Accuracy 0.841 \n",
      "\t 20 Loss = 530.653306568 Train_Accuracy 0.835 \n",
      "\t 30 Loss = 530.260717508 Train_Accuracy 0.837 \n",
      "\t 40 Loss = 494.074594696 Train_Accuracy 0.849 \n",
      "\t 50 Loss = 467.495589829 Train_Accuracy 0.843 \n",
      "\t 60 Loss = 417.525813517 Train_Accuracy 0.888 \n",
      "\t 70 Loss = 467.905168692 Train_Accuracy 0.85 \n",
      "\t 80 Loss = 448.10910172 Train_Accuracy 0.867 \n",
      "\t 86 Loss = 463.164703614 Train_Accuracy 0.85 \n",
      "2 0.863916666667\n",
      "\t 0 Loss = 2321.51512325 Train_Accuracy 0.078 \n",
      "\t 10 Loss = 591.32691904 Train_Accuracy 0.834 \n",
      "\t 20 Loss = 563.050477291 Train_Accuracy 0.835 \n",
      "\t 30 Loss = 517.3360104 Train_Accuracy 0.838 \n",
      "\t 40 Loss = 475.482853972 Train_Accuracy 0.851 \n",
      "\t 50 Loss = 448.098234269 Train_Accuracy 0.854 \n",
      "\t 60 Loss = 469.794611477 Train_Accuracy 0.855 \n",
      "\t 70 Loss = 475.436017301 Train_Accuracy 0.852 \n",
      "\t 78 Loss = 444.003739551 Train_Accuracy 0.855 \n",
      "3 0.86125\n",
      "\t 0 Loss = 2303.09639153 Train_Accuracy 0.122 \n",
      "\t 10 Loss = 585.075111654 Train_Accuracy 0.834 \n",
      "\t 20 Loss = 513.776340595 Train_Accuracy 0.851 \n",
      "\t 30 Loss = 499.087829947 Train_Accuracy 0.861 \n",
      "\t 40 Loss = 456.82751394 Train_Accuracy 0.871 \n",
      "\t 50 Loss = 504.295523845 Train_Accuracy 0.85 \n",
      "\t 60 Loss = 432.200707414 Train_Accuracy 0.871 \n",
      "\t 70 Loss = 454.590383224 Train_Accuracy 0.864 \n",
      "\t 80 Loss = 503.115007559 Train_Accuracy 0.852 \n",
      "\t 87 Loss = 457.166681294 Train_Accuracy 0.863 \n",
      "4 0.8615\n",
      "\t 0 Loss = 2306.09683806 Train_Accuracy 0.097 \n",
      "\t 10 Loss = 592.474782769 Train_Accuracy 0.826 \n",
      "\t 20 Loss = 515.905920148 Train_Accuracy 0.84 \n",
      "\t 30 Loss = 503.349113644 Train_Accuracy 0.846 \n",
      "\t 40 Loss = 539.723352085 Train_Accuracy 0.824 \n",
      "\t 50 Loss = 492.629260567 Train_Accuracy 0.85 \n",
      "\t 60 Loss = 392.171401697 Train_Accuracy 0.889 \n",
      "\t 70 Loss = 437.533148647 Train_Accuracy 0.875 \n",
      "\t 80 Loss = 433.938699551 Train_Accuracy 0.876 \n",
      "\t 90 Loss = 419.21382619 Train_Accuracy 0.867 \n",
      "\t 99 Loss = 431.730273024 Train_Accuracy 0.87 \n",
      "5 0.858083333333\n",
      "orientation= 1 0.861716666667\n",
      "\t 0 Loss = 2317.38286985 Train_Accuracy 0.062 \n",
      "\t 10 Loss = 341.193211613 Train_Accuracy 0.896 \n",
      "\t 20 Loss = 245.146832207 Train_Accuracy 0.929 \n",
      "\t 30 Loss = 229.683263092 Train_Accuracy 0.938 \n",
      "\t 40 Loss = 204.397386541 Train_Accuracy 0.931 \n",
      "\t 50 Loss = 191.044937423 Train_Accuracy 0.947 \n",
      "\t 60 Loss = 164.452144471 Train_Accuracy 0.953 \n",
      "\t 70 Loss = 211.403474056 Train_Accuracy 0.938 \n",
      "\t 80 Loss = 160.464719291 Train_Accuracy 0.955 \n",
      "\t 90 Loss = 183.193193636 Train_Accuracy 0.945 \n",
      "\t 99 Loss = 190.811473431 Train_Accuracy 0.945 \n",
      "1 0.945666666667\n",
      "\t 0 Loss = 2301.67913599 Train_Accuracy 0.061 \n",
      "\t 10 Loss = 297.096706407 Train_Accuracy 0.925 \n",
      "\t 20 Loss = 252.554495799 Train_Accuracy 0.934 \n",
      "\t 30 Loss = 239.290117644 Train_Accuracy 0.937 \n",
      "\t 40 Loss = 229.682193466 Train_Accuracy 0.933 \n",
      "\t 50 Loss = 169.298592854 Train_Accuracy 0.951 \n",
      "\t 60 Loss = 178.802148338 Train_Accuracy 0.947 \n",
      "\t 70 Loss = 189.089250554 Train_Accuracy 0.944 \n",
      "\t 80 Loss = 197.38855566 Train_Accuracy 0.943 \n",
      "\t 90 Loss = 153.342241162 Train_Accuracy 0.958 \n",
      "\t 99 Loss = 162.183047863 Train_Accuracy 0.952 \n",
      "2 0.9485\n",
      "\t 0 Loss = 2312.63856547 Train_Accuracy 0.108 \n",
      "\t 10 Loss = 292.717375608 Train_Accuracy 0.924 \n",
      "\t 20 Loss = 253.942852594 Train_Accuracy 0.928 \n",
      "\t 30 Loss = 227.035211545 Train_Accuracy 0.938 \n",
      "\t 40 Loss = 208.579709758 Train_Accuracy 0.942 \n",
      "\t 50 Loss = 207.127370373 Train_Accuracy 0.937 \n",
      "\t 60 Loss = 171.987218051 Train_Accuracy 0.953 \n",
      "\t 70 Loss = 192.383773379 Train_Accuracy 0.945 \n",
      "\t 80 Loss = 155.74353003 Train_Accuracy 0.958 \n",
      "\t 90 Loss = 156.35169084 Train_Accuracy 0.949 \n",
      "\t 99 Loss = 178.282241035 Train_Accuracy 0.951 \n",
      "3 0.946833333333\n",
      "\t 0 Loss = 2287.79264113 Train_Accuracy 0.155 \n",
      "\t 10 Loss = 303.689877687 Train_Accuracy 0.921 \n",
      "\t 20 Loss = 257.537772183 Train_Accuracy 0.936 \n",
      "\t 30 Loss = 197.734466245 Train_Accuracy 0.95 \n",
      "\t 40 Loss = 216.969509957 Train_Accuracy 0.938 \n",
      "\t 50 Loss = 173.203235327 Train_Accuracy 0.955 \n",
      "\t 60 Loss = 152.249051585 Train_Accuracy 0.955 \n",
      "\t 70 Loss = 167.286973084 Train_Accuracy 0.948 \n",
      "\t 80 Loss = 205.964433249 Train_Accuracy 0.937 \n",
      "\t 90 Loss = 202.569395701 Train_Accuracy 0.934 \n",
      "\t 99 Loss = 162.360643807 Train_Accuracy 0.951 \n",
      "4 0.948416666667\n",
      "\t 0 Loss = 2308.38353703 Train_Accuracy 0.139 \n",
      "\t 10 Loss = 340.401691485 Train_Accuracy 0.91 \n",
      "\t 20 Loss = 242.866757998 Train_Accuracy 0.925 \n",
      "\t 30 Loss = 220.306521001 Train_Accuracy 0.93 \n",
      "\t 40 Loss = 197.910017488 Train_Accuracy 0.937 \n",
      "\t 50 Loss = 200.174197343 Train_Accuracy 0.937 \n",
      "\t 60 Loss = 204.969910905 Train_Accuracy 0.944 \n",
      "\t 70 Loss = 168.352539541 Train_Accuracy 0.948 \n",
      "\t 80 Loss = 157.190503512 Train_Accuracy 0.956 \n",
      "\t 90 Loss = 162.196126771 Train_Accuracy 0.952 \n",
      "\t 99 Loss = 208.834931091 Train_Accuracy 0.937 \n",
      "5 0.950416666667\n",
      "orientation= 2 0.947966666667\n",
      "\t 0 Loss = 2310.28305577 Train_Accuracy 0.109 \n",
      "\t 10 Loss = 278.039205734 Train_Accuracy 0.922 \n",
      "\t 20 Loss = 174.67088352 Train_Accuracy 0.954 \n",
      "\t 30 Loss = 151.313422069 Train_Accuracy 0.958 \n",
      "\t 40 Loss = 163.442861556 Train_Accuracy 0.955 \n",
      "\t 50 Loss = 145.501613481 Train_Accuracy 0.965 \n",
      "\t 60 Loss = 134.116882085 Train_Accuracy 0.962 \n",
      "\t 70 Loss = 135.401517316 Train_Accuracy 0.965 \n",
      "\t 80 Loss = 133.629603976 Train_Accuracy 0.96 \n",
      "\t 90 Loss = 115.805932694 Train_Accuracy 0.965 \n",
      "\t 99 Loss = 121.917075265 Train_Accuracy 0.967 \n",
      "1 0.967833333333\n",
      "\t 0 Loss = 2296.75104515 Train_Accuracy 0.12 \n",
      "\t 10 Loss = 225.107072808 Train_Accuracy 0.938 \n",
      "\t 20 Loss = 182.832834382 Train_Accuracy 0.957 \n",
      "\t 30 Loss = 153.629869885 Train_Accuracy 0.96 \n",
      "\t 40 Loss = 129.502144174 Train_Accuracy 0.968 \n",
      "\t 50 Loss = 165.288441482 Train_Accuracy 0.955 \n",
      "\t 60 Loss = 135.459335407 Train_Accuracy 0.961 \n",
      "\t 70 Loss = 119.259858467 Train_Accuracy 0.965 \n",
      "\t 80 Loss = 136.98457565 Train_Accuracy 0.961 \n",
      "\t 90 Loss = 148.463895422 Train_Accuracy 0.965 \n",
      "\t 99 Loss = 125.793294908 Train_Accuracy 0.966 \n",
      "2 0.96625\n",
      "\t 0 Loss = 2320.43386005 Train_Accuracy 0.077 \n",
      "\t 10 Loss = 217.85452991 Train_Accuracy 0.954 \n",
      "\t 20 Loss = 214.178315483 Train_Accuracy 0.942 \n",
      "\t 30 Loss = 156.972447771 Train_Accuracy 0.965 \n",
      "\t 40 Loss = 139.277357015 Train_Accuracy 0.968 \n",
      "\t 50 Loss = 134.709642056 Train_Accuracy 0.96 \n",
      "\t 60 Loss = 163.875093406 Train_Accuracy 0.958 \n",
      "\t 70 Loss = 166.897201543 Train_Accuracy 0.953 \n",
      "\t 80 Loss = 107.675460437 Train_Accuracy 0.974 \n",
      "\t 90 Loss = 114.605935953 Train_Accuracy 0.966 \n",
      "\t 99 Loss = 117.846330642 Train_Accuracy 0.972 \n",
      "3 0.968833333333\n",
      "\t 0 Loss = 2323.28912004 Train_Accuracy 0.039 \n",
      "\t 10 Loss = 210.158768165 Train_Accuracy 0.958 \n",
      "\t 20 Loss = 166.52911545 Train_Accuracy 0.958 \n",
      "\t 30 Loss = 156.357485532 Train_Accuracy 0.963 \n",
      "\t 40 Loss = 150.511024139 Train_Accuracy 0.962 \n",
      "\t 50 Loss = 142.472312566 Train_Accuracy 0.963 \n",
      "\t 60 Loss = 149.668513728 Train_Accuracy 0.957 \n",
      "\t 70 Loss = 125.504199471 Train_Accuracy 0.962 \n",
      "\t 80 Loss = 100.439350836 Train_Accuracy 0.975 \n",
      "\t 90 Loss = 119.344603128 Train_Accuracy 0.967 \n",
      "\t 99 Loss = 120.228477542 Train_Accuracy 0.97 \n",
      "4 0.965666666667\n",
      "\t 0 Loss = 2312.05846182 Train_Accuracy 0.071 \n",
      "\t 10 Loss = 258.779723341 Train_Accuracy 0.93 \n",
      "\t 20 Loss = 201.51760504 Train_Accuracy 0.941 \n",
      "\t 30 Loss = 137.112508867 Train_Accuracy 0.973 \n",
      "\t 40 Loss = 167.294976226 Train_Accuracy 0.959 \n",
      "\t 50 Loss = 144.538743345 Train_Accuracy 0.959 \n",
      "\t 60 Loss = 120.290711652 Train_Accuracy 0.972 \n",
      "\t 70 Loss = 117.833919592 Train_Accuracy 0.967 \n",
      "\t 80 Loss = 108.934303424 Train_Accuracy 0.975 \n",
      "\t 90 Loss = 136.212442483 Train_Accuracy 0.957 \n",
      "\t 99 Loss = 107.332040259 Train_Accuracy 0.975 \n",
      "5 0.965333333333\n",
      "orientation= 4 0.966783333333\n",
      "\t 0 Loss = 2314.16192599 Train_Accuracy 0.106 \n",
      "\t 10 Loss = 164.685404778 Train_Accuracy 0.957 \n",
      "\t 20 Loss = 127.519059408 Train_Accuracy 0.966 \n",
      "\t 30 Loss = 138.033356922 Train_Accuracy 0.96 \n",
      "\t 40 Loss = 137.596904233 Train_Accuracy 0.962 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t 50 Loss = 83.4589377219 Train_Accuracy 0.973 \n",
      "\t 60 Loss = 90.3600821877 Train_Accuracy 0.975 \n",
      "\t 70 Loss = 99.6097972007 Train_Accuracy 0.974 \n",
      "\t 80 Loss = 87.3776622749 Train_Accuracy 0.971 \n",
      "\t 90 Loss = 104.251189792 Train_Accuracy 0.972 \n",
      "\t 99 Loss = 87.4454357134 Train_Accuracy 0.973 \n",
      "1 0.972583333333\n",
      "\t 0 Loss = 2316.89158987 Train_Accuracy 0.07 \n",
      "\t 10 Loss = 163.514623952 Train_Accuracy 0.959 \n",
      "\t 20 Loss = 159.678992165 Train_Accuracy 0.958 \n",
      "\t 30 Loss = 118.469763337 Train_Accuracy 0.976 \n",
      "\t 40 Loss = 111.915227624 Train_Accuracy 0.967 \n",
      "\t 50 Loss = 117.246383176 Train_Accuracy 0.966 \n",
      "\t 60 Loss = 102.895999709 Train_Accuracy 0.975 \n",
      "\t 70 Loss = 88.2990850082 Train_Accuracy 0.981 \n",
      "\t 80 Loss = 105.704107167 Train_Accuracy 0.971 \n",
      "\t 90 Loss = 107.855943739 Train_Accuracy 0.975 \n",
      "\t 99 Loss = 83.1591467615 Train_Accuracy 0.981 \n",
      "2 0.972416666667\n",
      "\t 0 Loss = 2288.196081 Train_Accuracy 0.164 \n",
      "\t 10 Loss = 185.91246824 Train_Accuracy 0.945 \n",
      "\t 20 Loss = 147.341722379 Train_Accuracy 0.966 \n",
      "\t 30 Loss = 136.510650703 Train_Accuracy 0.959 \n",
      "\t 40 Loss = 132.696527317 Train_Accuracy 0.966 \n",
      "\t 50 Loss = 102.42435017 Train_Accuracy 0.97 \n",
      "\t 60 Loss = 89.9695624501 Train_Accuracy 0.978 \n",
      "\t 70 Loss = 91.5599871204 Train_Accuracy 0.974 \n",
      "\t 80 Loss = 107.584693007 Train_Accuracy 0.972 \n",
      "\t 90 Loss = 108.637446571 Train_Accuracy 0.964 \n",
      "\t 99 Loss = 82.0910168008 Train_Accuracy 0.98 \n",
      "3 0.970833333333\n",
      "\t 0 Loss = 2315.73928491 Train_Accuracy 0.087 \n",
      "\t 10 Loss = 183.775388114 Train_Accuracy 0.956 \n",
      "\t 20 Loss = 161.663651162 Train_Accuracy 0.958 \n",
      "\t 30 Loss = 116.991093063 Train_Accuracy 0.968 \n",
      "\t 40 Loss = 141.160582084 Train_Accuracy 0.963 \n",
      "\t 50 Loss = 110.919749624 Train_Accuracy 0.968 \n",
      "\t 60 Loss = 111.119743549 Train_Accuracy 0.97 \n",
      "\t 70 Loss = 88.8371457899 Train_Accuracy 0.982 \n",
      "\t 80 Loss = 85.9917489465 Train_Accuracy 0.98 \n",
      "\t 90 Loss = 97.1518658764 Train_Accuracy 0.976 \n",
      "\t 99 Loss = 91.8876578034 Train_Accuracy 0.973 \n",
      "4 0.971833333333\n",
      "\t 0 Loss = 2317.91003163 Train_Accuracy 0.082 \n",
      "\t 10 Loss = 179.227793258 Train_Accuracy 0.949 \n",
      "\t 20 Loss = 156.437610113 Train_Accuracy 0.95 \n",
      "\t 30 Loss = 122.270277662 Train_Accuracy 0.968 \n",
      "\t 40 Loss = 110.630019625 Train_Accuracy 0.974 \n",
      "\t 50 Loss = 121.534476265 Train_Accuracy 0.967 \n",
      "\t 60 Loss = 111.755179483 Train_Accuracy 0.969 \n",
      "\t 70 Loss = 101.472678153 Train_Accuracy 0.974 \n",
      "\t 80 Loss = 110.919986838 Train_Accuracy 0.964 \n",
      "\t 90 Loss = 100.282525463 Train_Accuracy 0.97 \n",
      "\t 99 Loss = 76.3161087278 Train_Accuracy 0.985 \n",
      "5 0.97325\n",
      "orientation= 8 0.972183333333\n",
      "\t 0 Loss = 2307.31893624 Train_Accuracy 0.07 \n",
      "\t 10 Loss = 154.924989002 Train_Accuracy 0.957 \n",
      "\t 20 Loss = 94.9008863369 Train_Accuracy 0.978 \n",
      "\t 30 Loss = 112.148356048 Train_Accuracy 0.97 \n",
      "\t 40 Loss = 129.277028786 Train_Accuracy 0.959 \n",
      "\t 50 Loss = 90.6687322658 Train_Accuracy 0.969 \n",
      "\t 60 Loss = 84.4635681039 Train_Accuracy 0.978 \n",
      "\t 70 Loss = 84.7972160247 Train_Accuracy 0.981 \n",
      "\t 80 Loss = 89.5703758317 Train_Accuracy 0.97 \n",
      "\t 90 Loss = 83.0080593112 Train_Accuracy 0.981 \n",
      "\t 94 Loss = 72.4751324091 Train_Accuracy 0.983 \n",
      "1 0.975583333333\n",
      "\t 0 Loss = 2322.074871 Train_Accuracy 0.093 \n",
      "\t 10 Loss = 139.780244734 Train_Accuracy 0.964 \n",
      "\t 20 Loss = 133.487627739 Train_Accuracy 0.968 \n",
      "\t 30 Loss = 108.748843955 Train_Accuracy 0.972 \n",
      "\t 40 Loss = 87.6134387912 Train_Accuracy 0.973 \n",
      "\t 50 Loss = 117.148219142 Train_Accuracy 0.972 \n",
      "\t 60 Loss = 79.3747026 Train_Accuracy 0.976 \n",
      "\t 70 Loss = 84.3760064707 Train_Accuracy 0.975 \n",
      "\t 79 Loss = 65.6564876977 Train_Accuracy 0.979 \n",
      "2 0.973916666667\n",
      "\t 0 Loss = 2300.70022734 Train_Accuracy 0.146 \n",
      "\t 10 Loss = 163.572867048 Train_Accuracy 0.958 \n",
      "\t 20 Loss = 103.41415076 Train_Accuracy 0.977 \n",
      "\t 30 Loss = 100.327392354 Train_Accuracy 0.971 \n",
      "\t 40 Loss = 78.5235844365 Train_Accuracy 0.977 \n",
      "\t 50 Loss = 115.295828186 Train_Accuracy 0.965 \n",
      "\t 60 Loss = 84.0855837784 Train_Accuracy 0.977 \n",
      "\t 70 Loss = 91.2006195803 Train_Accuracy 0.976 \n",
      "\t 80 Loss = 66.9674486082 Train_Accuracy 0.983 \n",
      "\t 90 Loss = 74.1534011405 Train_Accuracy 0.986 \n",
      "\t 99 Loss = 70.6541872656 Train_Accuracy 0.978 \n",
      "3 0.974\n",
      "\t 0 Loss = 2310.82888345 Train_Accuracy 0.138 \n",
      "\t 10 Loss = 174.113128173 Train_Accuracy 0.952 \n",
      "\t 20 Loss = 129.229383353 Train_Accuracy 0.966 \n",
      "\t 30 Loss = 97.1500394085 Train_Accuracy 0.97 \n",
      "\t 40 Loss = 94.9896918245 Train_Accuracy 0.976 \n",
      "\t 50 Loss = 88.0560343698 Train_Accuracy 0.974 \n",
      "\t 60 Loss = 96.8934613343 Train_Accuracy 0.974 \n",
      "\t 64 Loss = 83.1409159536 Train_Accuracy 0.976 \n",
      "4 0.974833333333\n",
      "\t 0 Loss = 2298.53995891 Train_Accuracy 0.083 \n",
      "\t 10 Loss = 122.502814372 Train_Accuracy 0.965 \n",
      "\t 20 Loss = 133.162054422 Train_Accuracy 0.969 \n",
      "\t 30 Loss = 88.1896712425 Train_Accuracy 0.977 \n",
      "\t 40 Loss = 80.3374571469 Train_Accuracy 0.978 \n",
      "\t 50 Loss = 99.5527726144 Train_Accuracy 0.969 \n",
      "\t 60 Loss = 104.106125701 Train_Accuracy 0.977 \n",
      "\t 70 Loss = 82.7924439822 Train_Accuracy 0.972 \n",
      "\t 77 Loss = 87.0697275656 Train_Accuracy 0.98 \n",
      "5 0.975166666667\n",
      "orientation= 16 0.9747\n",
      "\t 0 Loss = 2312.02792377 Train_Accuracy 0.038 \n",
      "\t 10 Loss = 136.199824447 Train_Accuracy 0.959 \n",
      "\t 20 Loss = 103.153209994 Train_Accuracy 0.973 \n",
      "\t 30 Loss = 75.3531591761 Train_Accuracy 0.981 \n",
      "\t 40 Loss = 80.9319230429 Train_Accuracy 0.978 \n",
      "\t 50 Loss = 82.7497858185 Train_Accuracy 0.978 \n",
      "\t 60 Loss = 75.56207805 Train_Accuracy 0.983 \n",
      "\t 70 Loss = 72.2336752642 Train_Accuracy 0.98 \n",
      "\t 80 Loss = 90.8449108644 Train_Accuracy 0.979 \n",
      "\t 90 Loss = 60.2667372527 Train_Accuracy 0.981 \n",
      "\t 99 Loss = 44.4594963126 Train_Accuracy 0.99 \n",
      "1 0.9745\n",
      "\t 0 Loss = 2311.31850994 Train_Accuracy 0.09 \n",
      "\t 10 Loss = 110.383661843 Train_Accuracy 0.971 \n",
      "\t 20 Loss = 104.906847797 Train_Accuracy 0.966 \n",
      "\t 30 Loss = 72.8990691713 Train_Accuracy 0.982 \n",
      "\t 40 Loss = 62.1562492603 Train_Accuracy 0.981 \n",
      "\t 50 Loss = 88.7088379744 Train_Accuracy 0.97 \n",
      "\t 60 Loss = 73.6438166115 Train_Accuracy 0.971 \n",
      "\t 60 Loss = 69.8220199905 Train_Accuracy 0.978 \n",
      "2 0.974083333333\n",
      "\t 0 Loss = 2298.67872448 Train_Accuracy 0.134 \n",
      "\t 10 Loss = 112.406609259 Train_Accuracy 0.971 \n",
      "\t 20 Loss = 96.9872846473 Train_Accuracy 0.978 \n",
      "\t 30 Loss = 88.6050155134 Train_Accuracy 0.969 \n",
      "\t 40 Loss = 88.3063807499 Train_Accuracy 0.975 \n",
      "\t 50 Loss = 71.9888049972 Train_Accuracy 0.979 \n",
      "\t 59 Loss = 100.141107214 Train_Accuracy 0.969 \n",
      "3 0.97625\n",
      "\t 0 Loss = 2300.19853384 Train_Accuracy 0.125 \n",
      "\t 10 Loss = 112.865507579 Train_Accuracy 0.97 \n",
      "\t 20 Loss = 85.6160860841 Train_Accuracy 0.979 \n",
      "\t 30 Loss = 92.8130019881 Train_Accuracy 0.976 \n",
      "\t 40 Loss = 89.4433080862 Train_Accuracy 0.977 \n",
      "\t 50 Loss = 57.4448626223 Train_Accuracy 0.985 \n",
      "\t 60 Loss = 64.4364059831 Train_Accuracy 0.987 \n",
      "\t 61 Loss = 92.6328194115 Train_Accuracy 0.973 \n",
      "4 0.977\n",
      "\t 0 Loss = 2306.83191888 Train_Accuracy 0.131 \n",
      "\t 10 Loss = 121.672407289 Train_Accuracy 0.966 \n",
      "\t 20 Loss = 99.4523774502 Train_Accuracy 0.975 \n",
      "\t 30 Loss = 93.0425070411 Train_Accuracy 0.977 \n",
      "\t 40 Loss = 68.2755264084 Train_Accuracy 0.982 \n",
      "\t 50 Loss = 69.7688932992 Train_Accuracy 0.979 \n",
      "\t 60 Loss = 82.7839565122 Train_Accuracy 0.978 \n",
      "\t 64 Loss = 74.9948346464 Train_Accuracy 0.976 \n",
      "5 0.977083333333\n",
      "orientation= 32 0.975783333333\n"
     ]
    }
   ],
   "source": [
    "# we will go all the way to 32 orientations\n",
    "# for this problem you only needed to go up to 8\n",
    "base_model=LogisticGDClassifier()\n",
    "K=5\n",
    "block_size=4\n",
    "orientations=[1,2,4,8,16,32]\n",
    "\n",
    "val_accuracy=[]\n",
    "for orientation in orientations:\n",
    "    model=mnist.ImageFeatureModel(base_model,size=block_size,orientations=orientation)\n",
    "    accuracy=validate_model(model,K,images,labels)\n",
    "    val_accuracy.append(accuracy)\n",
    "    print(\"orientation=\",orientation,accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T03:14:27.037248Z",
     "start_time": "2018-03-02T03:14:27.029228Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best number of orientations 32 0.975783333333\n"
     ]
    }
   ],
   "source": [
    "val_accuracy=np.array(val_accuracy)\n",
    "best_idx=val_accuracy.argmax()\n",
    "best_orientations=orientations[best_idx]\n",
    "print(\"best number of orientations\",best_orientations,val_accuracy[best_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T03:14:27.257344Z",
     "start_time": "2018-03-02T03:14:27.042262Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1d6936e45f8>]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD8CAYAAABw1c+bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nljEOfjOzjHHwm5lljIPfzCxj/j/edanCCmNEQAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d68fbb6588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(orientations,val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 5.2 </div>\n",
    "\n",
    "Fit the model with the optimal number of orientations to the full MNIST data set and estimate its accuracy on the MNIST test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-02T03:16:52.002722Z",
     "start_time": "2018-03-02T03:14:27.259850Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t 0 Loss = 2311.97266492 Train_Accuracy 0.051 \n",
      "\t 10 Loss = 109.761835374 Train_Accuracy 0.971 \n",
      "\t 20 Loss = 94.7885214674 Train_Accuracy 0.972 \n",
      "\t 30 Loss = 72.1905535949 Train_Accuracy 0.976 \n",
      "\t 40 Loss = 85.3511515461 Train_Accuracy 0.978 \n",
      "\t 50 Loss = 73.6007107665 Train_Accuracy 0.978 \n",
      "\t 60 Loss = 69.4628982731 Train_Accuracy 0.983 \n",
      "\t 70 Loss = 57.7171451484 Train_Accuracy 0.986 \n",
      "\t 80 Loss = 57.5551867118 Train_Accuracy 0.983 \n",
      "\t 90 Loss = 64.7193259523 Train_Accuracy 0.978 \n",
      "\t 99 Loss = 47.4168959232 Train_Accuracy 0.987 \n",
      "Test Accuracy =  0.9816\n"
     ]
    }
   ],
   "source": [
    "model=mnist.ImageFeatureModel(base_model,size=block_size,orientations=best_orientations)\n",
    "model.fit(images,labels)\n",
    "Y_pred=model.predict(test_images)\n",
    "print(\"Test Accuracy = \", np.mean(Y_pred==test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
