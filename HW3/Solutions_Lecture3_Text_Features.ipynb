{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Feature Extraction Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will implement another slight variation of the `tfidf`  document distance definition using **sublinear** document counts.\n",
    "\n",
    "We will then compare it to the `sklearn` implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T10:01:27.971494Z",
     "start_time": "2018-02-02T10:01:13.684602Z"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import os\n",
    "import pickle\n",
    "from collections import Counter \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import sys \n",
    "sys.path.append(\"../..\")\n",
    "from E4525_ML import text # you must have saved the file text.py into the E4525_ML directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T10:01:27.976450Z",
     "start_time": "2018-02-02T10:01:27.972439Z"
    }
   },
   "outputs": [],
   "source": [
    "raw_data_dir=r\"../../raw/C50/C50train\" # original data set used for training\n",
    "data_dir    =r\"../../data/C50/\"  # directory to save intermediate results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T15:56:22.749141Z",
     "start_time": "2018-01-10T15:56:22.744141Z"
    }
   },
   "source": [
    "#### Convenience Function Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few functions carried over from the Text_Features notebook that we will need during this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T10:01:27.994499Z",
     "start_time": "2018-02-02T10:01:27.978458Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_text(filename,stop): \n",
    "    porter_stemmer = PorterStemmer()\n",
    "    file=open(filename)\n",
    "    lines=file.readlines()\n",
    "    text_str=\" \".join(lines).replace(\"\\n\",\" \").lower()\n",
    "    stem_list=text.stem_tokenizer(text_str)\n",
    "    used_list=[token for token in stem_list if token not in stop]\n",
    "    return used_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T10:01:28.020567Z",
     "start_time": "2018-02-02T10:01:27.996503Z"
    }
   },
   "outputs": [],
   "source": [
    "def text_2_set(filename,stop_words):\n",
    "    stems=process_text(filename,stop_words)\n",
    "    return set(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T10:01:28.040621Z",
     "start_time": "2018-02-02T10:01:28.022573Z"
    }
   },
   "outputs": [],
   "source": [
    "def text_2_counts(filename,stop_words):\n",
    "    stems=process_text(filename,stop_words)\n",
    "    return Counter(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T10:01:28.061725Z",
     "start_time": "2018-02-02T10:01:28.042627Z"
    }
   },
   "outputs": [],
   "source": [
    "def corpus_word_counts(documents,stop):\n",
    "    counts=Counter()\n",
    "    for filename in documents[\"filename\"]:   \n",
    "        print(\"processing...\",filename)\n",
    "        bag=text_2_set(filename,stop)\n",
    "        for word in bag:\n",
    "            counts[word]+=1\n",
    "    return pd.DataFrame.from_dict(counts,orient=\"index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 0 </div>\n",
    "\n",
    "1. Download the  [Reuters 50](https://archive.ics.uci.edu/ml/datasets/Reuter_50_50) collection of texts. Save it on the `raw` data directory.\n",
    "\n",
    "    You should end up with this directory structure structure:\n",
    "    \n",
    "    raw/\n",
    "        C50/\n",
    "            C50train/\n",
    "            C50test/\n",
    "            \n",
    "1. Run to completion the [Text Feature Extraction](./Text_Features.ipynb) notebook. This will generate the document lists, and word count statistics. Make sure to run any of the sections are are meant to be run only once.\n",
    "1. Save the text.py python module into the `E4525_ML` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T16:01:13.535141Z",
     "start_time": "2018-01-10T16:01:13.525141Z"
    }
   },
   "source": [
    "## Implement TF-IDF document Distance with Sublinear Growth "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T15:54:33.146141Z",
     "start_time": "2018-01-10T15:54:33.138141Z"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 1.1 </div>\n",
    "\n",
    "Read the list of documents in the file `C50_documents.csv`  from the data directory `data_dir` into a `documents` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T10:01:28.105795Z",
     "start_time": "2018-02-02T10:01:28.063681Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>document_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../../raw/C50/C50train/ScottHillis/253868newsM...</td>\n",
       "      <td>ScottHillis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../../raw/C50/C50train/ScottHillis/305692newsM...</td>\n",
       "      <td>ScottHillis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../../raw/C50/C50train/ScottHillis/340736newsM...</td>\n",
       "      <td>ScottHillis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../../raw/C50/C50train/ScottHillis/140340newsM...</td>\n",
       "      <td>ScottHillis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../../raw/C50/C50train/ScottHillis/126593newsM...</td>\n",
       "      <td>ScottHillis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      filename        label\n",
       "document_id                                                                \n",
       "0            ../../raw/C50/C50train/ScottHillis/253868newsM...  ScottHillis\n",
       "1            ../../raw/C50/C50train/ScottHillis/305692newsM...  ScottHillis\n",
       "2            ../../raw/C50/C50train/ScottHillis/340736newsM...  ScottHillis\n",
       "3            ../../raw/C50/C50train/ScottHillis/140340newsM...  ScottHillis\n",
       "4            ../../raw/C50/C50train/ScottHillis/126593newsM...  ScottHillis"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents_filename=data_dir+\"/C50_documents.csv\"\n",
    "\n",
    "documents=pd.read_csv(documents_filename,index_col=\"document_id\")\n",
    "documents.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 1.2 </div>\n",
    "\n",
    "Create a list of stop works by calling the function `text.stop_words` from the `E4525.text` python module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T10:01:28.123842Z",
     "start_time": "2018-02-02T10:01:28.107801Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words=text.stop_words()\n",
    "stop_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 1.3 </div>\n",
    "\n",
    "Using pandas, read  the word count (term frequencies) file generated by the Text_Features notebook\n",
    "The file is called \"corpus_word_counts.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T10:01:28.179991Z",
     "start_time": "2018-02-02T10:01:28.125847Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>batteri</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>raytheon</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intimid</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>commentari</th>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>issu</th>\n",
       "      <td>689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            count\n",
       "word             \n",
       "batteri        11\n",
       "raytheon        7\n",
       "intimid         9\n",
       "commentari     20\n",
       "issu          689"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts_filename=data_dir+\"corpus_word_counts.csv\"\n",
    "\n",
    "word_counts=pd.read_csv(word_counts_filename,index_col=\"word\")\n",
    "word_counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 1.4 </div>\n",
    "Create a variable $V$ with the vocabulary size  and a variable named $C$ with the total number of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T10:01:28.193026Z",
     "start_time": "2018-02-02T10:01:28.181997Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size 28060\n",
      "Corpus Size 2500\n"
     ]
    }
   ],
   "source": [
    "# Vocabulary size\n",
    "V=len(word_counts)\n",
    "print(\"Vocabulary Size\",V)\n",
    "# Corpus size\n",
    "C=len(documents)\n",
    "print(\"Corpus Size\",C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T17:28:58.416141Z",
     "start_time": "2018-01-10T17:28:58.407141Z"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 1.5 </div>\n",
    "Compute the smoothed inverse document counts, defined as\n",
    "$$\n",
    "    \\textrm{idf}_i =  \\log\\left( \\frac{1+C}{1+\\textrm{n}_i}\\right) + 1\n",
    "$$\n",
    "\n",
    "where $n_i$ is the number of documents in corpus where word $i$ appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T10:01:28.264214Z",
     "start_time": "2018-02-02T10:01:28.196035Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>batteri</th>\n",
       "      <td>6.339539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>raytheon</th>\n",
       "      <td>6.745004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intimid</th>\n",
       "      <td>6.521861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>commentari</th>\n",
       "      <td>5.779923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>issu</th>\n",
       "      <td>2.287754</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               count\n",
       "word                \n",
       "batteri     6.339539\n",
       "raytheon    6.745004\n",
       "intimid     6.521861\n",
       "commentari  5.779923\n",
       "issu        2.287754"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idfs=np.log((1+C)/(1+word_counts))+1\n",
    "idfs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"> We set up a few documents for comparison</div>\n",
    "\n",
    "[HINT] Code below assumes that the variable `documents`  is the list of documents you read in problem 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T10:01:28.272237Z",
     "start_time": "2018-02-02T10:01:28.265218Z"
    }
   },
   "outputs": [],
   "source": [
    "# document indexes we will use for comparison\n",
    "document1=0 \n",
    "document2=1\n",
    "document3=105\n",
    "\n",
    "# document filenames\n",
    "filename1=documents[\"filename\"][document1]\n",
    "filename2=documents[\"filename\"][document2]\n",
    "filename3=documents[\"filename\"][document3] # this will be from a different author"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 1.6 </div>\n",
    "    Compute the word counts for `documents1`,`document2` and `document3`, using the `text_2_count` function defined at the beginning of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T10:01:28.392556Z",
     "start_time": "2018-02-02T10:01:28.275246Z"
    }
   },
   "outputs": [],
   "source": [
    "count1=text_2_counts(filename1,stop_words)\n",
    "count2=text_2_counts(filename2,stop_words)\n",
    "count3=text_2_counts(filename3,stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T18:30:16.684141Z",
     "start_time": "2018-01-10T18:30:16.677141Z"
    }
   },
   "source": [
    "The function below computes the normalized product of  `tfidf`  vectors.\n",
    "Where the `tfidf` vector is defined as follows\n",
    "$$\n",
    "    w_{k} = \\textrm{idf_k} * c_{k}\n",
    "$$\n",
    "where  $c_{k}$ is the number of times that word $k$ appears in document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T10:01:28.416620Z",
     "start_time": "2018-02-02T10:01:28.398574Z"
    }
   },
   "outputs": [],
   "source": [
    "def product_tfidf(count1,count2,idfs):\n",
    "    sum1=0.0\n",
    "    sum_cross=0.0\n",
    "    for key in count1:\n",
    "        if key not in idfs.index:\n",
    "            idf=0\n",
    "            print(f\"key {key} not found\")\n",
    "        else:\n",
    "            idf=idfs.loc[key][\"count\"]\n",
    "        w1=idf*count1[key]\n",
    "        w2=idf*count2[key]\n",
    "        sum1+=(w1)**2\n",
    "        sum_cross+=w1*w2\n",
    "    sum2=0.0\n",
    "    for key in count2:\n",
    "        if key not in idfs.index:\n",
    "            idf=0\n",
    "            print(f\"key {key} not found\")\n",
    "        else:\n",
    "            idf=idfs.loc[key][\"count\"]\n",
    "        w2=idf*count2[key]\n",
    "        sum2+=w2**2\n",
    "    return sum_cross/np.sqrt(sum1*sum2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems unlikely that 20 occurrences of a term in a document truly carry $20\\times$ the significance of a single occurrence. And alternative (see the [Information Retrieval book](https://nlp.stanford.edu/IR-book/html/htmledition/sublinear-tf-scaling-1.html)) is to use a function\n",
    "to *tamper* the growth of the word counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T18:45:53.854141Z",
     "start_time": "2018-01-10T18:45:53.846141Z"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 1.6 </div>\n",
    "Create a function named `sublinear_product_tfidf`.\n",
    "It should compute the normalized product of `tfidf` vectors as above but using a **`sublinear`** measure of  the word counts, defined as:\n",
    "\\begin{align}\n",
    "    w_k  &= idf_k * (1+\\log c_k)  &\\textrm{if}\\,\\, c_k &>0 \\\\\n",
    "    w_k  &= 0                    &\\textrm{if}\\,\\, c_k &=0 \\\\\n",
    "\\end{align}\n",
    "where $c_k$ is the raw word count for word $k$.\n",
    "\n",
    "[HINT] Probably easiest to copy and modify slightly the function  `product_idf` above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T10:01:28.445698Z",
     "start_time": "2018-02-02T10:01:28.420633Z"
    }
   },
   "outputs": [],
   "source": [
    "def sublinear(c):\n",
    "    if c>0:\n",
    "        return 1+np.log(c)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# function is identical to product_tfidf except for the three lines with comments.\n",
    "def sublinear_product_tfidf(count1,count2,idfs):\n",
    "    sum1=0.0\n",
    "    sum_cross=0.0\n",
    "    for key in count1:\n",
    "        if key not in idfs.index:\n",
    "            idf=0\n",
    "            print(f\"key {key} not found\")\n",
    "        else:\n",
    "            idf=idfs.loc[key][\"count\"]\n",
    "        w1=idf*sublinear(count1[key])     # change here!      \n",
    "        w2=idf*sublinear(count2[key])     # change here!\n",
    "        sum1+=(w1)**2\n",
    "        sum_cross+=w1*w2\n",
    "    sum2=0.0\n",
    "    for key in count2:\n",
    "        if key not in idfs.index:\n",
    "            idf=0\n",
    "            print(f\"key {key} not found\")\n",
    "        else:\n",
    "            idf=idfs.loc[key][\"count\"]\n",
    "        w2=idf*sublinear(count2[key])   # change here!\n",
    "        sum2+=w2**2\n",
    "    return sum_cross/np.sqrt(sum1*sum2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 1.7 </div>\n",
    "Compute the sublinear normalized product (similarity) for `document1` with itself, verify that the product is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T10:01:28.522903Z",
     "start_time": "2018-02-02T10:01:28.451715Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sublinear_product_tfidf(count1,count1,idfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T18:55:57.862141Z",
     "start_time": "2018-01-10T18:55:57.854141Z"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 1.8 </div>\n",
    "Compute the sublinear normalized products between \n",
    "1. `document1` and `document2`\n",
    "2. `document1` and `document3`\n",
    "3. `document2` and `document3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T10:01:28.685336Z",
     "start_time": "2018-02-02T10:01:28.524908Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.11227509153067713, 0.041620049051990064, 0.034038033058980455)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sublinear_product_tfidf(count1,count2,idfs),sublinear_product_tfidf(count1,count3,idfs),sublinear_product_tfidf(count2,count3,idfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison to  `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T23:27:17.319716Z",
     "start_time": "2018-01-23T23:27:17.285700Z"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 2.1 </div>\n",
    "store the value of the function `text.stem_tokenizer` from the module `text.py` into variable named `tokenizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T10:01:28.690348Z",
     "start_time": "2018-02-02T10:01:28.687339Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer=text.stem_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 2.2 </div>\n",
    "\n",
    "set up  an instance of [`sklearn.TfidfVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)  so that it generates `tfidf` vectors using sublinear growth.\n",
    "\n",
    "[Hint] \n",
    "1. Read carefully the  long list of options on the constructor of `TfidfVectorizer`\n",
    "2. Do not forget to set the `input`, `tokenizer` and `stop_word` arguments.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T10:01:28.710401Z",
     "start_time": "2018-02-02T10:01:28.693356Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidfVectorizer=TfidfVectorizer(input=\"filename\",\n",
    "                                tokenizer=tokenizer,\n",
    "                                stop_words=stop_words,\n",
    "                                sublinear_tf=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 2.3 </div>\n",
    "Generate the matrix $X$ of `tfidf` representations for each document in our corpus (this may take a bit of time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T10:02:12.525262Z",
     "start_time": "2018-02-02T10:01:28.713410Z"
    }
   },
   "outputs": [],
   "source": [
    "X=tfidfVectorizer.fit_transform(documents[\"filename\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T19:03:06.301141Z",
     "start_time": "2018-01-10T19:03:06.295141Z"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 2.4 </div>\n",
    "Compute the dot product between `document1` and `document2` using their vector (`X`) representation. \n",
    "\n",
    "Compare to the result produced by the `sublinear_product_tfidf`\n",
    "function you just wrote. They should be nearly identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T10:02:12.572389Z",
     "start_time": "2018-02-02T10:02:12.527268Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.11227509153067713, 0.11227509153067713)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(X[document1],X[document2].T)[0,0],sublinear_product_tfidf(count1,count2,idfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Trained models for Reuse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 3.1 </div>\n",
    "In the data directory `data_dir`:\n",
    "1. Save vectorizer to a `pickle` called \"tfidf_sublinear_vectorizer.p\"\n",
    "2. Save sublinear `tfidf1` features to a file called \"tfidf_sublinear_features.p\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T10:02:12.703738Z",
     "start_time": "2018-02-02T10:02:12.574395Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer_filename=   data_dir+\"/tfidf_sublinear_vectorizer.p\"\n",
    "tfidf_features_filename=     data_dir+\"/tfidf_sublinear_features.p\"\n",
    "\n",
    "\n",
    "pickle.dump(tfidfVectorizer, open( tfidf_vectorizer_filename, \"wb\" ) )\n",
    "pickle.dump(X,              open( tfidf_features_filename, \"wb\" ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 3.2 </div>\n",
    "Make sure you can read those files again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T10:02:12.747855Z",
     "start_time": "2018-02-02T10:02:12.707748Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='filename',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs',...', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~', '``', \"''\"],\n",
       "         strip_accents=None, sublinear_tf=True,\n",
       "         token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=<function stem_tokenizer at 0x7fa202f41598>,\n",
       "         use_idf=True, vocabulary=None),\n",
       " <2500x28060 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 496636 stored elements in Compressed Sparse Row format>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfVectorizer2=pickle.load( open(tfidf_vectorizer_filename, \"rb\" ))\n",
    "X2=pickle.load(open(tfidf_features_filename, \"rb\" ) )\n",
    "tfidfVectorizer2,X2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "497px",
    "left": "0px",
    "right": "auto",
    "top": "107px",
    "width": "314px"
   },
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
