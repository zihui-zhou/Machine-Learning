{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Feature Extraction Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we will implement another slight variation of the `tfidf`  document distance definition using **sublinear** document counts.\n",
    "\n",
    "We will then compare it to the `sklearn` implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T10:07:13.605374Z",
     "start_time": "2018-02-02T10:07:08.599075Z"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import os\n",
    "import pickle\n",
    "from collections import Counter \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import sys \n",
    "sys.path.append(\"../../\")\n",
    "from E4525_ML import text # you must have saved the file text.py into the E4525_ML directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T10:07:13.610388Z",
     "start_time": "2018-02-02T10:07:13.606375Z"
    }
   },
   "outputs": [],
   "source": [
    "raw_data_dir=r\"../../raw/C50/C50train\" # original data set used for training\n",
    "data_dir    =r\"../../data/C50/\"  # directory to save intermediate results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T15:56:22.749141Z",
     "start_time": "2018-01-10T15:56:22.744141Z"
    }
   },
   "source": [
    "#### Convenience Function Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few functions carried over from the Text_Features notebook that we will need during this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T10:07:13.639464Z",
     "start_time": "2018-02-02T10:07:13.613395Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_text(filename,stop): \n",
    "    porter_stemmer = PorterStemmer()\n",
    "    file=open(filename)\n",
    "    lines=file.readlines()\n",
    "    text_str=\" \".join(lines).replace(\"\\n\",\" \").lower()\n",
    "    stem_list=text.stem_tokenizer(text_str)\n",
    "    used_list=[token for token in stem_list if token not in stop]\n",
    "    return used_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T10:07:13.655547Z",
     "start_time": "2018-02-02T10:07:13.641470Z"
    }
   },
   "outputs": [],
   "source": [
    "def text_2_set(filename,stop_words):\n",
    "    stems=process_text(filename,stop_words)\n",
    "    return set(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T10:07:13.671550Z",
     "start_time": "2018-02-02T10:07:13.657512Z"
    }
   },
   "outputs": [],
   "source": [
    "def text_2_counts(filename,stop_words):\n",
    "    stems=process_text(filename,stop_words)\n",
    "    return Counter(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T10:07:13.689597Z",
     "start_time": "2018-02-02T10:07:13.673560Z"
    }
   },
   "outputs": [],
   "source": [
    "def corpus_word_counts(documents,stop):\n",
    "    counts=Counter()\n",
    "    for filename in documents[\"filename\"]:   \n",
    "        print(\"processing...\",filename)\n",
    "        bag=text_2_set(filename,stop)\n",
    "        for word in bag:\n",
    "            counts[word]+=1\n",
    "    return pd.DataFrame.from_dict(counts,orient=\"index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 0 </div>\n",
    "\n",
    "1. Download the  [Reuters 50](https://archive.ics.uci.edu/ml/datasets/Reuter_50_50) collection of texts. Save it on the `raw` data directory.\n",
    "\n",
    "    You should end up with this directory structure structure:\n",
    "    \n",
    "    raw/\n",
    "        C50/\n",
    "            C50train/\n",
    "            C50test/\n",
    "            \n",
    "1. Run to completion the [Text Feature Extraction](./Text_Features.ipynb) notebook. This will generate the document lists, and word count statistics. Make sure to run any of the sections are are meant to be run only once.\n",
    "1. Save the text.py python module into the `E4525_ML` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T16:01:13.535141Z",
     "start_time": "2018-01-10T16:01:13.525141Z"
    }
   },
   "source": [
    "## Implement TF-IDF document Distance with Sublinear Growth "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T15:54:33.146141Z",
     "start_time": "2018-01-10T15:54:33.138141Z"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 1.1 </div>\n",
    "\n",
    "Read the list of documents in the file `C50_documents.csv`  from the data directory `data_dir` into a `documents` variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>../../raw/C50/C50train/RobinSidel/147604newsML...</td>\n",
       "      <td>RobinSidel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>../../raw/C50/C50train/RobinSidel/196812newsML...</td>\n",
       "      <td>RobinSidel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>../../raw/C50/C50train/RobinSidel/219316newsML...</td>\n",
       "      <td>RobinSidel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>../../raw/C50/C50train/RobinSidel/251225newsML...</td>\n",
       "      <td>RobinSidel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>../../raw/C50/C50train/RobinSidel/177958newsML...</td>\n",
       "      <td>RobinSidel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   document_id                                           filename       label\n",
       "0            0  ../../raw/C50/C50train/RobinSidel/147604newsML...  RobinSidel\n",
       "1            1  ../../raw/C50/C50train/RobinSidel/196812newsML...  RobinSidel\n",
       "2            2  ../../raw/C50/C50train/RobinSidel/219316newsML...  RobinSidel\n",
       "3            3  ../../raw/C50/C50train/RobinSidel/251225newsML...  RobinSidel\n",
       "4            4  ../../raw/C50/C50train/RobinSidel/177958newsML...  RobinSidel"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = pd.read_csv(data_dir + 'C50_documents.csv')\n",
    "documents.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 1.2 </div>\n",
    "\n",
    "Create a list of stop works by calling the function `text.stop_words` from the `E4525.text` python module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " \"'d\",\n",
       " \"'ll\",\n",
       " \"'re\",\n",
       " \"'s\",\n",
       " \"'ve\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " '^',\n",
       " '_',\n",
       " '`',\n",
       " '``',\n",
       " 'a',\n",
       " 'about',\n",
       " 'abov',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'ani',\n",
       " 'are',\n",
       " 'aren',\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'becaus',\n",
       " 'been',\n",
       " 'befor',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'could',\n",
       " 'couldn',\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " 'do',\n",
       " 'doe',\n",
       " 'doesn',\n",
       " 'don',\n",
       " 'down',\n",
       " 'dure',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'ha',\n",
       " 'had',\n",
       " 'hadn',\n",
       " 'hasn',\n",
       " 'have',\n",
       " 'haven',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'herself',\n",
       " 'hi',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " 'it',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'might',\n",
       " 'mightn',\n",
       " 'more',\n",
       " 'most',\n",
       " 'must',\n",
       " 'mustn',\n",
       " 'my',\n",
       " 'myself',\n",
       " \"n't\",\n",
       " 'need',\n",
       " 'needn',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'onc',\n",
       " 'onli',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ourselv',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'sha',\n",
       " 'shan',\n",
       " 'she',\n",
       " 'should',\n",
       " 'shouldn',\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'their',\n",
       " 'them',\n",
       " 'themselv',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'thi',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'veri',\n",
       " 'wa',\n",
       " 'wasn',\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'whi',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'will',\n",
       " 'with',\n",
       " 'wo',\n",
       " 'won',\n",
       " 'would',\n",
       " 'wouldn',\n",
       " 'y',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yourself',\n",
       " 'yourselv',\n",
       " '{',\n",
       " '|',\n",
       " '}',\n",
       " '~'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = text.stop_words()\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 1.3 </div>\n",
    "\n",
    "Using pandas, read  the word count (term frequencies) file generated by the Text_Features notebook\n",
    "The file is called \"corpus_word_counts.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>paid</th>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>declin</th>\n",
       "      <td>572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new</th>\n",
       "      <td>1472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>difficulti</th>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>market</th>\n",
       "      <td>1473</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            count\n",
       "word             \n",
       "paid          169\n",
       "declin        572\n",
       "new          1472\n",
       "difficulti     67\n",
       "market       1473"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = pd.read_csv(data_dir + '/corpus_word_counts.csv', index_col='word')\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 1.4 </div>\n",
    "Create a variable $V$ with the vocabulary size  and a variable named $C$ with the total number of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the vocabulary size is: 28131\n",
      "the total number of documents: 2500\n"
     ]
    }
   ],
   "source": [
    "V = corpus.shape[0]\n",
    "C = documents.shape[0]\n",
    "print('the vocabulary size is: ' + str(V))\n",
    "print('the total number of documents: ' + str(C))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T17:28:58.416141Z",
     "start_time": "2018-01-10T17:28:58.407141Z"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 1.5 </div>\n",
    "Compute the smoothed inverse document counts, defined as\n",
    "$$\n",
    "    \\textrm{idf}_i =  \\log\\left( \\frac{1+C}{1+\\textrm{n}_i}\\right) + 1\n",
    "$$\n",
    "\n",
    "where $n_i$ is the number of documents in corpus where word $i$ appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               count\n",
      "word                \n",
      "paid        3.688647\n",
      "declin      2.473560\n",
      "new         1.529390\n",
      "difficulti  4.604938\n",
      "market      1.528711\n"
     ]
    }
   ],
   "source": [
    "idf = np.log((1+C)/(1+corpus)) + 1 # Using the dataframe directly\n",
    "print(idf.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\"> We set up a few documents for comparison</div>\n",
    "\n",
    "[HINT] Code below assumes that the variable `documents`  is the list of documents you read in problem 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T10:07:13.823955Z",
     "start_time": "2018-02-02T10:07:13.691604Z"
    }
   },
   "outputs": [],
   "source": [
    "# document indexes we will use for comparison\n",
    "document1=0 \n",
    "document2=1\n",
    "document3=105\n",
    "\n",
    "# document filenames\n",
    "filename1=documents[\"filename\"][document1]\n",
    "filename2=documents[\"filename\"][document2]\n",
    "filename3=documents[\"filename\"][document3] # this will be from a different author"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 1.6 </div>\n",
    "    Compute the word counts for `documents1`,`document2` and `document3`, using the `text_2_count` function defined at the beginning of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'revco': 20, 'big': 20, 'b': 19, 'said': 11, 'share': 9, 'offer': 8, 'store': 8, 'compani': 8, 'inc.': 7, 'buy': 5, 'combin': 5, 'profit': 5, 'chain': 4, 'stock': 4, 'per': 4, \"''\": 4, 'sale': 4, 'oper': 4, 'drugstor': 3, 'month': 3, 'deal': 3, 'rite': 3, 'aid': 3, 'sign': 3, 'unit': 3, 'although': 3, 'trade': 3, 'new': 3, 'margin': 3, 'distribut': 3, 'centr': 3, 'monday': 2, 'agre': 2, 'region': 2, 'million': 2, 'transact': 2, '15': 2, 'reject': 2, 'last': 2, 'abl': 2, 'hoven': 2, 'drug': 2, 'board': 2, 'latest': 2, 'tender': 2, 'acquisit': 2, 'industri': 2, 'corp.': 2, 'agreement': 2, 'billion': 2, 'view': 2, 'cent': 2, 'financi': 2, 'togeth': 2, 'base': 2, 'potenti': 2, 'like': 2, 'manag': 2, 'care': 2, 'overlap': 2, 'georgia': 2, 'group': 2, 'year': 2, 'price': 2, 'invest': 2, 'strauss': 2, 'also': 2, 'two': 2, 'earlier': 2, 'eckerd': 2, 'southeastern': 2, 'state': 2, 'giant': 1, 'd.': 1, 'sweeten': 1, 'takeov': 1, 'valu': 1, '380': 1, 'call': 1, 'twinsburg': 1, 'ohio-bas': 1, 'outstand': 1, 'common': 1, '17.25': 1, 'unsolicit': 1, 'excit': 1, 'b.': 1, 'pleas': 1, 'bring': 1, 'process': 1, 'fast': 1, 'success': 1, 'conclus': 1, 'dwayn': 1, 'presid': 1, 'chief': 1, 'execut': 1, 'offic': 1, 'nation': 1, 'second-': 1, '10th-largest': 1, 'director': 1, 'unanim': 1, 'approv': 1, 'recommend': 1, 'sharehold': 1, 'bessem': 1, 'ala.-bas': 1, 'mark': 1, 'rapidli': 1, 'consolid': 1, 'west': 1, 'coast': 1, 'thrifti': 1, 'payless': 1, '1.4': 1, 'thrift': 1, 'j.c.': 1, 'penney': 1, 'co.': 1, 'august': 1, 'fay': 1, '285': 1, 'initi': 1, 'recent': 1, '16': 1, 'sold': 1, 'origin': 1, '12.5': 1, '29.50': 1, 'york': 1, 'exchang': 1, '17': 1, '43.75': 1, 'afternoon': 1, 'nasdaq': 1, 'resourc': 1, 'technolog': 1, 'expertis': 1, 'market': 1, 'capabl': 1, 'grow': 1, 'custom': 1, 'increas': 1, 'among': 1, 'effici': 1, 'allow': 1, 'spread': 1, 'cost': 1, 'larger': 1, 'work': 1, 'ensur': 1, 'smooth': 1, 'transit': 1, 'mani': 1, 'fight': 1, 'problem': 1, 'accompani': 1, 'burgeon': 1, 'clout': 1, 'health': 1, 'huge': 1, 'discount': 1, 'retail': 1, 'hurt': 1, 'deterior': 1, 'earn': 1, 'disappoint': 1, 'wall': 1, 'street': 1, 'expert': 1, 'say': 1, 'well-plac': 1, 'captur': 1, 'southeast': 1, 'take': 1, 'advantag': 1, 'underus': 1, 'close': 1, 'integr': 1, 'realli': 1, 'quickli': 1, 'eric': 1, 'bosshard': 1, 'midwest': 1, 'research-maxu': 1, 'began': 1, 'tumbl': 1, 'due': 1, 'difficulti': 1, 'calcul': 1, 'impact': 1, 'lower': 1, 'prescript': 1, 'paid': 1, 'provid': 1, 'heaviliy': 1, 'comput': 1, 'system': 1, 'better': 1, 'track': 1, 'peopl': 1, 'familiar': 1, 'newli': 1, 'expand': 1, '60': 1, 'percent': 1, 'capac': 1, 'analyst': 1, 'one': 1, 'first': 1, 'step': 1, 'assess': 1, 'especi': 1, '180': 1, '160': 1, 'get': 1, 'real': 1, 'john': 1, 'advisori': 1, 'firm': 1, 'revis': 1, 'extend': 1, 'nov': 1, '15.': 1, 'set': 1, 'expir': 1, 'litig': 1, 'withdrawn': 1, 'haggl': 1, 'term': 1, 'confidenti': 1, 'took': 1, 'peek': 1, 'book': 1, 'appar': 1, 'saw': 1, 'met': 1, 'weekend': 1, 'accept': 1, 'higher': 1, 'detail': 1, 'immedi': 1, 'known': 1, 'bidder': 1, 'declin': 1, 'comment': 1, '397': 1, 'throughout': 1, '2,202': 1, '14': 1, 'contigu': 1, 'midwestern': 1, 'eastern': 1, 'seen': 1, 'signific': 1, 'involv': 1, 'fail': 1, 'pact': 1, '1.8': 1, 'collaps': 1, 'regul': 1, 'express': 1, 'concern': 1, 'union': 1, 'rais': 1, 'consum': 1})\n",
      "Counter({'mattel': 20, 'tyco': 17, 'said': 13, 'market': 8, 'toy': 7, \"''\": 7, 'year': 6, 'compani': 5, 'propos': 5, 'share': 5, 'barad': 5, 'transact': 5, 'million': 4, 'combin': 4, 'hasbro': 4, 'analyst': 4, 'inc.': 3, 'boy': 3, 'car': 3, 'categori': 3, 'stock': 3, 'repres': 3, 'percent': 3, 'toymak': 3, 'forc': 3, 'potenti': 3, '1995': 3, 'also': 3, 'deal': 3, 'expand': 2, 'union': 2, 'hot': 2, 'wheel': 2, 'miniatur': 2, 'matchbox': 2, 'one': 2, 'jill': 2, 'telephon': 2, 'interview': 2, 'follow': 2, 'effort': 2, '2': 2, 'nation': 2, 'bid': 2, 'concern': 2, 'expect': 2, 'complet': 2, 'merger': 2, 'industri': 2, 'acquisit': 2, 'mani': 2, 'smith': 2, 'barney': 2, 'krutick': 2, 'presenc': 2, 'brand': 2, 'two': 2, 'antitrust': 2, 'look': 2, 'base': 2, 'revenu': 2, 'sale': 2, 'billion': 2, 'cite': 2, 'lead': 2, 'long': 2, 'get': 2, 'regul': 2, 'work': 2, 'fee': 2, 'seek': 1, 'agre': 1, 'buy': 1, 'third-rank': 1, '755': 1, 'monday': 1, 'barbi': 1, 'doll': 1, 'fisher-pric': 1, 'sesam': 1, 'street': 1, 'radio-control': 1, 'vehicl': 1, 'realli': 1, 'believ': 1, 'busi': 1, 'import': 1, 'never': 1, 'big': 1, 'presid': 1, 'agreement': 1, 'sharehold': 1, 'receiv': 1, 'valu': 1, '12.50': 1, 'per': 1, '78': 1, 'premium': 1, 'friday': 1, 'close': 1, 'price': 1, 'jump': 1, '4.50': 1, '11.50': 1, 'fell': 1, '75': 1, 'cent': 1, '29.875': 1, 'new': 1, 'york': 1, 'exchang': 1, 'unsuccess': 1, 'earlier': 1, 'acquir': 1, 'rival': 1, 'largest': 1, 'withdraw': 1, 'unwelcom': 1, 'due': 1, 'widespread': 1, 'effect': 1, 'competit': 1, 'fist': 1, 'half': 1, '1997': 1, 'add': 1, 'earn': 1, 'first': 1, 'board': 1, 'approv': 1, 'discuss': 1, 'began': 1, 'sept.': 1, '29': 1, 'meet': 1, 'chief': 1, 'execut': 1, 'offic': 1, 'gari': 1, 'baughman': 1, 'met': 1, 'oct.': 1, '14.': 1, 'expert': 1, 'accomplish': 1, 'goal': 1, 'fail': 1, 'someth': 1, 'tri': 1, 'desir': 1, 'boost': 1, 'outstand': 1, 'worldwid': 1, 'well-known': 1, 'fulli': 1, 'realis': 1, 'chairman': 1, 'john': 1, 'amerman': 1, 'statement': 1, 'although': 1, 'biggest': 1, 'issu': 1, 'signific': 1, 'note': 1, 'hold': 1, 'less': 1, '20': 1, 'highli': 1, 'fragment': 1, 'still': 1, 'low': 1, 'even': 1, 'breakdown': 1, 'mt': 1, 'laurel': 1, 'n.j.': 1, '709': 1, 'el': 1, 'segundo': 1, 'calif.': 1, '3.6': 1, 'maker': 1, '2.8': 1, 'implic': 1, 'overlap': 1, 'unit': 1, 'think': 1, 'go': 1, 'process': 1, 'ok': 1, 'debacl': 1, 'anyth': 1, 'point': 1, 'want': 1, 'identifi': 1, 'money-los': 1, 'culmin': 1, 'turnaround': 1, 'last': 1, 'cut': 1, '10': 1, 'part': 1, 'restructur': 1, 'aim': 1, 'return': 1, 'profit': 1, 'deep': 1, 'pocket': 1, 'global': 1, 'reach': 1, 'easili': 1, 'capitalis': 1, 'product': 1, 'clearli': 1, 'way': 1, 'enhanc': 1, 'growth': 1, 'intern': 1, 'fairli': 1, 'cleaned-up': 1, 'situat': 1, 'strengthen': 1, 'posit': 1, 'plush': 1, 'leader': 1, 'includ': 1, '40': 1, 'payabl': 1, 'anoth': 1, 'lure': 1, 'higher': 1, 'call': 1, 'break-up': 1, '15': 1, 'within': 1, 'declin': 1, 'specifi': 1, 'anticip': 1, 'cost': 1, 'save': 1, 'associ': 1, 'say': 1, 'lot': 1, 'cover': 1, 'earli': 1, 'assess': 1, 'impact': 1})\n",
      "Counter({'said': 8, 'new': 7, 'appl': 6, 'powerbook': 6, 'model': 6, '1400': 5, 'comput': 4, 'famili': 4, 'cd-rom': 4, 'drive': 4, 'corp.': 4, 'screen': 3, 'intern': 3, 'machin': 3, 'go': 3, 'portabl': 2, 'line': 2, 'mid-novemb': 2, 'price': 2, '2,500': 2, 'bigger': 2, 'back': 2, 'compet': 2, 'offer': 2, 'analyst': 2, 'steven': 2, 'base': 2, 'microprocessor': 2, 'inc.': 1, 'unveil': 1, 'monday': 1, 'fill': 1, 'critic': 1, 'gap': 1, 'product': 1, 'schedul': 1, 'sold': 1, 'start': 1, 'featur': 1, 'built-in': 1, 'design': 1, 'appeal': 1, 'small': 1, 'offic': 1, 'user': 1, 'student': 1, 'lack': 1, 'held': 1, 'busi': 1, 'compaq': 1, 'dell': 1, 'put': 1, 'leadership': 1, 'role': 1, 'get': 1, 'game': 1, \"''\": 1, 'bruce': 1, 'market': 1, 'research': 1, 'data': 1, 'also': 1, 'plan': 1, 'introduc': 1, 'anoth': 1, 'seri': 1, 'first': 1, 'half': 1, '1997': 1, 'advanc': 1, 'multimedia': 1, 'commun': 1, 'capabl': 1, 'compani': 1, 'releas': 1, 'specif': 1, 'scrambl': 1, 'retool': 1, 'predecessor': 1, '5300': 1, 'recal': 1, 'last': 1, 'spring': 1, 'sever': 1, 'defect': 1, 'includ': 1, 'faulti': 1, 'ac': 1, 'power': 1, 'connector': 1, 'display': 1, 'case': 1, 'easili': 1, 'crack': 1, 'replac': 1, 'mid-rang': 1, 'powerpc': 1, 'faster': 1, 'make': 1, 'compar': 1, 'perform': 1, 'top-sel': 1, 'vendor': 1, 'tough': 1, 'time': 1, 'low-pric': 1, 'second-ti': 1, 'pc': 1, 'manufactur': 1, 'memori': 1, 'option': 1, 'disk-driv': 1, 'size': 1, 'sell': 1, 'estim': 1, '4,000': 1, 'retail': 1, 'unit': 1, 'state': 1, 'spokeswoman': 1, 'entry-level': 1, 'although': 1, 'sale': 1, 'limit': 1, 'suppli': 1, 'year': 1})\n"
     ]
    }
   ],
   "source": [
    "wc_d1 = text_2_counts(filename1,stopwords)\n",
    "wc_d2 = text_2_counts(filename2,stopwords)\n",
    "wc_d3 = text_2_counts(filename3,stopwords)\n",
    "print(wc_d1)\n",
    "print(wc_d2)\n",
    "print(wc_d3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classical Tf-Idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T18:30:16.684141Z",
     "start_time": "2018-01-10T18:30:16.677141Z"
    }
   },
   "source": [
    "The function below computes the normalized product of  `tfidf`  vectors.\n",
    "Where the `tfidf` vector is defined as follows\n",
    "$$\n",
    "    w_{k} = \\textrm{idf_k} * c_{k}\n",
    "$$\n",
    "where  $c_{k}$ is the number of times that word $k$ appears in document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T10:07:13.824959Z",
     "start_time": "2018-02-02T10:07:08.665Z"
    }
   },
   "outputs": [],
   "source": [
    "def product_tfidf(count1,count2,idfs):\n",
    "    sum1=0.0\n",
    "    sum_cross=0.0\n",
    "    for key in count1:\n",
    "        if key not in idfs.index:\n",
    "            idf=0\n",
    "            print(f\"key {key} not found\")\n",
    "        else:\n",
    "            idf=idfs.loc[key][\"count\"]\n",
    "        w1=idf*count1[key]\n",
    "        w2=idf*count2[key]\n",
    "        sum1+=(w1)**2\n",
    "        sum_cross+=w1*w2\n",
    "    sum2=0.0\n",
    "    for key in count2:\n",
    "        if key not in idfs.index:\n",
    "            idf=0\n",
    "            print(f\"key {key} not found\")\n",
    "        else:\n",
    "            idf=idfs.loc[key][\"count\"]\n",
    "        w2=idf*count2[key]\n",
    "        sum2+=w2**2\n",
    "    return sum_cross/np.sqrt(sum1*sum2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-Linear Tf-Idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems unlikely that 20 occurrences of a term in a document truly carry $20\\times$ the significance of a single occurrence. And alternative (see the [Information Retrieval book](https://nlp.stanford.edu/IR-book/html/htmledition/sublinear-tf-scaling-1.html)) is to use a function\n",
    "to *tamper* the growth of the word counts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T18:45:53.854141Z",
     "start_time": "2018-01-10T18:45:53.846141Z"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 1.6 </div>\n",
    "Create a function named `sublinear_product_tfidf`.\n",
    "It should compute the normalized product of `tfidf` vectors as above but using a **`sublinear`** measure of  the word counts, defined as:\n",
    "\\begin{align}\n",
    "    w_k  &= idf_k * (1+\\log c_k)  &\\textrm{if}\\,\\, c_k &>0 \\\\\n",
    "    w_k  &= 0                    &\\textrm{if}\\,\\, c_k &=0 \\\\\n",
    "\\end{align}\n",
    "where $c_k$ is the raw word count for word $k$.\n",
    "\n",
    "[HINT] Probably easiest to copy and modify slightly the function  `product_idf` above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T10:01:28.445698Z",
     "start_time": "2018-02-02T10:01:28.420633Z"
    }
   },
   "outputs": [],
   "source": [
    "def count(c): # *** These lines are important\n",
    "    if c>0:\n",
    "        return 1+np.log(c)\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def product_sl_tfidf(count1,count2,idfs):\n",
    "    sum1=0.0\n",
    "    sum_cross=0.0\n",
    "    for key in count1:\n",
    "        if key not in idfs.index:\n",
    "            idf=0\n",
    "            print(f\"key {key} not found\")\n",
    "        else:\n",
    "            idf=idfs.loc[key][\"count\"]\n",
    "        w1=idf*count(count1[key])\n",
    "        w2=idf*count(count2[key])\n",
    "        sum1+=(w1)**2\n",
    "        sum_cross+=w1*w2\n",
    "    sum2=0.0\n",
    "    for key in count2:\n",
    "        if key not in idfs.index:\n",
    "            idf=0\n",
    "            print(f\"key {key} not found\")\n",
    "        else:\n",
    "            idf=idfs.loc[key][\"count\"]\n",
    "        w2=idf*count(count2[key])\n",
    "        sum2+=w2**2\n",
    "    return sum_cross/np.sqrt(sum1*sum2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 1.7 </div>\n",
    "Compute the sublinear normalized product (similarity) for `document1` with itself, verify that the product is 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T10:01:28.522903Z",
     "start_time": "2018-02-02T10:01:28.451715Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product_sl_tfidf(wc_d1,wc_d1,idf) # computer the similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T18:55:57.862141Z",
     "start_time": "2018-01-10T18:55:57.854141Z"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 1.8 </div>\n",
    "Compute the sublinear normalized products between \n",
    "1. `document1` and `document2`\n",
    "2. `document1` and `document3`\n",
    "3. `document2` and `document3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T10:01:28.685336Z",
     "start_time": "2018-02-02T10:01:28.524908Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12787626323711282\n",
      "0.04982868267860495\n",
      "0.0352982411561606\n"
     ]
    }
   ],
   "source": [
    "snp1 = product_sl_tfidf(wc_d1,wc_d2,idf) # Need the sublinear function be defined according to the range\n",
    "snp2 = product_sl_tfidf(wc_d1,wc_d3,idf)\n",
    "snp3 = product_sl_tfidf(wc_d2,wc_d3,idf)\n",
    "print(snp1)\n",
    "print(snp2)\n",
    "print(snp3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison to  `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-23T23:27:17.319716Z",
     "start_time": "2018-01-23T23:27:17.285700Z"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 2.1 </div>\n",
    "store the value of the function `text.stem_tokenizer` from the module `text.py` into variable named `tokenizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T10:01:28.690348Z",
     "start_time": "2018-02-02T10:01:28.687339Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = text.stem_tokenizer # text.py abbreviated as text to apply the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 2.2 </div>\n",
    "\n",
    "set up  an instance of [`sklearn.TfidfVectorizer`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)  so that it generates `tfidf` vectors using sublinear growth.\n",
    "\n",
    "[Hint] \n",
    "1. Read carefully the  long list of options on the constructor of `TfidfVectorizer`\n",
    "2. Do not forget to set the `input`, `tokenizer` and `stop_word` arguments.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T10:01:28.710401Z",
     "start_time": "2018-02-02T10:01:28.693356Z"
    }
   },
   "outputs": [],
   "source": [
    "TfidV = TfidfVectorizer(input = 'filename', tokenizer = tokenizer, stop_words = stopwords, sublinear_tf = True)\n",
    "# Since the function generates tfidf vector using sublinear growth, setup the sublinear_tf to true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 2.3 </div>\n",
    "Generate the matrix $X$ of `tfidf` representations for each document in our corpus (this may take a bit of time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T10:02:12.525262Z",
     "start_time": "2018-02-02T10:01:28.713410Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhouzihui/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['becau'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "X = TfidV.fit_transform(documents['filename']) # .fit_transform to apply the function on the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-10T19:03:06.301141Z",
     "start_time": "2018-01-10T19:03:06.295141Z"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 2.4 </div>\n",
    "Compute the dot product between `document1` and `document2` using their vector (`X`) representation. \n",
    "\n",
    "Compare to the result produced by the `sublinear_product_tfidf`\n",
    "function you just wrote. They should be nearly identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T10:02:12.572389Z",
     "start_time": "2018-02-02T10:02:12.527268Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1278762632371129\n",
      "0.12787626323711282\n"
     ]
    }
   ],
   "source": [
    "# both print(X[document1].shape), print(X[document2].shape) give (1, 28131)\n",
    "print(np.dot(X[document1], X[document2].T)[0,0]) # Gives the only value in the 2-dimensional matrix\n",
    "print(product_sl_tfidf(wc_d1,wc_d2,idf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Trained models for Reuse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 3.1 </div>\n",
    "In the data directory `data_dir`:\n",
    "1. Save vectorizer to a `pickle` called \"tfidf_sublinear_vectorizer.p\"\n",
    "2. Save sublinear `tfidf1` features to a file called \"tfidf_sublinear_features.p\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T10:02:12.703738Z",
     "start_time": "2018-02-02T10:02:12.574395Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Save vectorizer to a `pickle` called \"tfidf_sublinear_vectorizer.p\"\n",
    "pickle.dump(TfidV, open(data_dir + '/tfidf_sublinear_vectorizer.p', 'wb'))\n",
    "\n",
    "# 2. Save sublinear `tfidf1` features to a file called \"tfidf_sublinear_features.p\"\n",
    "pickle.dump(X, open(data_dir + '/tfidf_sublinear_features.p', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<div class=\"alert alert-block alert-info\"> Problem 3.2 </div>\n",
    "Make sure you can read those files again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-02T10:02:12.747855Z",
     "start_time": "2018-02-02T10:02:12.707748Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "vectorizer = pickle.load( open( data_dir + '/tfidf_sublinear_vectorizer.p', \"rb\" ) )\n",
    "tfidf1 = pickle.load( open( data_dir + '/tfidf_sublinear_features.p', \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
      "                input='filename', lowercase=True, max_df=1.0, max_features=None,\n",
      "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
      "                smooth_idf=True,\n",
      "                stop_words={'!', '#', '$', '%', '&', \"'\", \"'d\", \"'ll\", \"'re\",\n",
      "                            \"'s\", \"'ve\", '(', ')', '*', '+', ',', '-', '.', '/',\n",
      "                            ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']',\n",
      "                            '^', ...},\n",
      "                strip_accents=None, sublinear_tf=True,\n",
      "                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "                tokenizer=<function stem_tokenizer at 0x1a1af67d90>,\n",
      "                use_idf=True, vocabulary=None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<2500x28131 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 483542 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(vectorizer)\n",
    "tfidf1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "497px",
    "left": "0px",
    "right": "auto",
    "top": "107px",
    "width": "314px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
